
TODO: 

- Refactor to modular code files: Done 
- remove sessionfile dependency: Done 
- Handle different forms of neuron data + speed up multi neuron processing: in process, need other forms of data for testing, make a load session function, with separate classes for handling diff types of inputs also make an optional manual loader so that people can load data manually (maybe integrate this with baseloader ? )
- evaluate cache policy ? 
- Unit Tests 
- Integration Tests  
- implement some way to save the model, maybe use existing bayesian model on pytorch or something
- Write function specs + documentation 


Questions/Queries: 

- How can we enesure that perforance of both previous code bases is identical to the new one ? The model is non-deterministic 
- Is there real data in other formats that I can use to test ? 


- Code Specific Notes: 
    - Remove SingleNameSpace() instances (change to list / dict / python class/ pandas data frame)
    - Reduce Session File dependency across code base. --> change to df + dicts : done  
    - Refactor (while True) in synthetic_spiketrain(): done 
    - Make an enum for categories and conditions: done
    - incorporate trial interval class stuff inside decoding instead of making it a user facing function 
    - use python3 bc sklearn has some float issue



- Michele's notes: scalability 1 neuron vs multiple neurons , different tpyes of input, pynapple code


resources/other notes: 
- style guide: https://google.github.io/styleguide/pyguide.html 
pitt cluster: 
- sbatch , crc-usage, bgfs: storage 
- ssh prg101@h2p.crc.pitt.edu 

Session file usage across code: 

trial interval _ToTimeStamp: 

top level (CalculateDecodingForSingleNeuron): 
| 
    lickDelay -> sessionfile.trials.response, sessionfile.trials.go, sessionfile.trials.starts, sessionfile.trials.go 


    cachedCalculateClusterAccuracy -> 
    sessionfile.trim[clust].trimmed_trials, 
        active_trials = trialsPerDayLoaded[sessionfile.meta.animal][sessionfile.meta.day_of_training]
    |   Sklearn_grid_search_bw -> 
        | getAllconditions 
        | getLogISIs -> 
            | util.getSpikeTimes -> 
            | interval._toTimeStamp -> 
    |   CacheLogISIs -> sessionfile.meta.length_in_trials, sessionfile.meta.fs
        | toTimeStamp -> sessionfile.trials.starts, sessionfile.trials.response
        | getSpikeTimes -> sessionfile.spikes.times, sessionfile.spikes.clusters
    | getAllConditions -> sessionfile.meta.task, 
        | getTrialSet -> sessionfile.trim[clust].trimmed_trials, sessionfile.meta.animal, sessionfile.meta.day_of_training, sessionfile.trials.target, sessionfile.trials.go, sessionfile.trials.response, sessionfile.trials.starts, sessionfile.meta.fs, sessionfile.trials.laser_stimulation, sessionfile.meta.first_reversal_trial, sessionfile.meta.length_in_trials
    | calculate_weights -> 
        | getAllConditions: up 
    | K_fold_strat -> sessionfile.trials.target, sessionfile.trials.go
    | cachedTrainDecodingAlgorithm -> sessionfile.meta.length_in_trials 
        | splitbyConditions -> getAllConditions only
    | cachedCalculateAccuracyonFold -> 
        | getAllConditions -> up 
        | cachedPredictTrial ->  not used but passed in 
    generateDateString -> 

getAllCOnditions and getTrialSet should be in the same file, 

exhaustive list of needed info from sessionfile: 

- sessionfile.trials.response: nd array of floats
- sesionfile.trials.go: bool arr
- sessionfile.trials.starts: float array 
- sessionfile.trials.target: bool array 
- sessionfile.trials.laser_stimulation --> only if trialSet is laser-on or laser-off 

- sessionfile.trim (For trimmed trials) 

- sessionfile.meta.animal : str
- sessionfile.meta.fs : int
- sessionfile.meta.day_of_recording : int
- sessionfile.meta.day_of_training : int
- sessionfie.meta.date : str 

- sessionfile.meta.length_in_trials : int32

- sessionfile.meta.region : str 
- sessionfile.meta.task : str 

- sessionfile.meta.first_reversal_trial: optional attribute 


- sessionfile.spikes.times 
- sessionfile.spikes.clusters 
- sessionfiles.spikes.amplitudes




